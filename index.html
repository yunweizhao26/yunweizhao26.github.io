<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yunwei Zhao</title>

  <meta name="author" content="Yunwei Zhao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Yunwei Zhao
                  </p>
                  <p>I'm a master student in Computer Science at <a href="https://ai.google/research">Cornell
                      University</a>. I am actively seeking PhD opportunities for the fall of 2024!
                  </p>
                  <p>
                    As a highly self-motivated and independent person, my time at the University of Washington has been
                    marked by significant accomplishments. At University of Washington,
                    my work
                    under Professor <a href="https://homes.cs.washington.edu/~swang/">Sheng Wang</a> focused on
                    designing robust evaluation
                    protocols of LLMs from human feedback and advancing multilingual
                    and multimodal translation methods for science (<a
                      href="https://academic.oup.com/bioinformatics/article/39/Supplement_1/i504/7210443">Gemini</a>,
                    <a href="">Crit2SQL</a>,
                    and <a href="https://www.nature.com/articles/s41467-023-36476-2">BioTranslator</a>).
                    This research
                    experience solidified my interest and commitment to adaptive
                    and trustworthy NLP. I have also worked with Professor
                    <a href="https://scholar.google.com/citations?user=uLsDDUMAAAAJ&hl=en">Richard Anderson</a> on
                    user-centric
                    technology that aids
                    people in developing regions (<a href="">eKichabi V2</a>).
                    All of these experiences have solidified my interest in
                    user-centric NLP systems, which enhance my passion for the
                    broader field of human-AI interaction.
                  </p>
                  <p>
                    At Cornell, I have been actively working on research, startup projects,
                    and internships to design user-centric NLP systems
                    in practice and integrate them into real products to
                    study human-LM interaction.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:yz26cn@cornell.edu">Email</a> &nbsp;/&nbsp;
                    <a href="data/CV_YUNWEI_B.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="data/rs.pdf"><u>Research Statement</u></a> &nbsp;/&nbsp;
                    <a href="https://github.com/yz26cn/">Github</a> &nbsp;/&nbsp;
                    Google Scholar &nbsp;/&nbsp;
                    Twitter
                  </p>
                </td>
                <td style="padding:2.5%; width:40%; max-width:40%">
                  <div class="image-hover">
                    <a href="images/JacobZhao.png">
                      <img alt="profile photo" src="images/JacobZhao.png" class="hoverZoomLink">
                      <div class="hover-text">I already have hundreds of ideas waiting
                        for me to research on and realize,
                        and I am highly self-motivated, continuously learning,
                        and practicing. Recently, I have been diving into full-stack practice for NLP for healthcare,
                        transitioning from 2D images to 3D assets, exploring mixed reality, and creating an
                        out-of-the-box, interactive, multi-modal, multi-lingual communicative system. It will be the
                        same for the next semester. I am open to new ideas and topics.
                      </div>
                    </a>
                  </div>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    My research interests lie in adaptability, trustworthiness, and human-AI interaction. I aim to
                    bridge the gap in user engagement by integrating real-time human feedback with AI design to develop
                    AI agents that can learn from human behaviors, communicate naturally, and explain their processes. I
                    am also broadly interested in the intersection of Human-Computer Interaction and Natural Language
                    Processing.
                  </p>
                  <p>
                    So far, I have two authored papers under review: one is eKichabi V2 with Professor Richard Anderson
                    where I actively involved in app design and implementation, user testing, data analysis, and
                    manuscript refinement; the other one is Crit2SQL where I contribute to dataset analysis and
                    development with Professor Sheng Wang.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='smerf_image'><video width=100% muted autoplay loop>
                        <source src="images/smerf.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/smerf.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function smerf_start() {
                      document.getElementById('smerf_image').style.opacity = "1";
                    }

                    function smerf_stop() {
                      document.getElementById('smerf_image').style.opacity = "0";
                    }
                    smerf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://smerf-3d.github.io/">
                    <span class="papertitle">SMERF: Streamable Memory Efficient Radiance Fields for Real-Time
                      Large-Scene Exploration</span>
                  </a>
                  <br>
                  <a href="http://www.stronglyconvex.com/about.html">Daniel Duckworth*</a>,
                  <a href="https://phogzone.com/">Peter Hedman*</a>,
                  <a href="https://creiser.github.io/">Christian Reiser</a>,
                  <a href="">Peter Zhizhin</a>,
                  <a href="">Jean-François Thibert</a>,
                  <a href="https://lucic.ai/">Mario Lučić</a>,
                  <a href="https://szeliski.org/">Richard Szeliski</a>,
                  <strong>Jonathan T. Barron</strong>
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <a href="https://smerf-3d.github.io/">project page</a>
                  /
                  <a href="https://www.youtube.com/watch?v=zhO8iUBpnCc">video</a>
                  /
                  <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
                  <p></p>
                  <p>
                    Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and
                    smartphones at 60 FPS.
                  </p>
                </td>
              </tr>

              <tr onmouseout="eclipse_stop()" onmouseover="eclipse_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='eclipse_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/eclipse_after.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/eclipse_before.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function eclipse_start() {
                      document.getElementById('eclipse_image').style.opacity = "1";
                    }

                    function eclipse_stop() {
                      document.getElementById('eclipse_image').style.opacity = "0";
                    }
                    eclipse_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dorverbin.github.io/eclipse">
                    <span class="papertitle">Eclipse: Disambiguating Illumination and Materials using Unintended
                      Shadows</span>
                  </a>
                  <br>
                  <a href="https://dorverbin.github.io/">Dor Verbin</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                  <a href="https://phogzone.com/">Peter Hedman</a>, <br>
                  <strong>Jonathan T. Barron</strong>,
                  <a href="Todd Zickler">Todd Zickler</a>,
                  <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <a href="https://dorverbin.github.io/eclipse">project page</a>
                  /
                  <a href="https://www.youtube.com/watch?v=amQLGyza3EU">video</a>
                  /
                  <a href="https://arxiv.org/abs/2305.16321">arXiv</a>
                  <p></p>
                  <p>
                    Shadows cast by unobserved occluders provide a high-frequency cue for recovering illumination and
                    materials.
                  </p>
                </td>
              </tr>


              <tr onmouseout="alignerf_stop()" onmouseover="alignerf_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='alignerf_image'>
                      <img src='images/alignerf_after.jpg' width="160">
                    </div>
                    <img src='images/alignerf_before.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function alignerf_start() {
                      document.getElementById('alignerf_image').style.opacity = "1";
                    }

                    function alignerf_stop() {
                      document.getElementById('alignerf_image').style.opacity = "0";
                    }
                    alignerf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://yifanjiang19.github.io/alignerf">
                    <span class="papertitle">AligNeRF: High-Fidelity Neural Radiance Fields via Alignment-Aware
                      Training</span>
                  </a>
                  <br>
                  <a href="https://yifanjiang.net/">Yifan Jiang</a>,
                  <a href="https://phogzone.com/">Peter Hedman</a>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>,
                  <a href="https://ir1d.github.io/">Dejia Xu</a>, <br>
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
                  <a href="https://tianfan.info/">Tianfan Xue</a>
                  <br>
                  <em>CVPR</em>, 2023
                  <br>
                  <a href="https://yifanjiang19.github.io/alignerf">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2211.09682">arXiv</a>
                  <p></p>
                  <p>
                    Accounting for misalignment due to scene motion or calibration errors improves NeRF reconstruction
                    quality.
                  </p>
                </td>
              </tr>

              <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dreamfusion_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/dreamfusion.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/dreamfusion.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function dreamfusion_start() {
                      document.getElementById('dreamfusion_image').style.opacity = "1";
                    }

                    function dreamfusion_stop() {
                      document.getElementById('dreamfusion_image').style.opacity = "0";
                    }
                    dreamfusion_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dreamfusion3d.github.io/">
                    <span class="papertitle">DreamFusion: Text-to-3D using 2D Diffusion</span>
                  </a>
                  <br>
                  <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>,
                  <a href="https://www.ajayj.com/">Ajay Jain</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>
                  <br>
                  <em>ICLR</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Outstanding Paper
                      Award)</strong></font>
                  <br>
                  <a href="https://dreamfusion3d.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a>
                  <p></p>
                  <p>
                    We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D
                    generative modeling.
                  </p>
                </td>
              </tr>

            </tbody>
          </table> -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Miscellanea</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:40px;width:25%;vertical-align:middle">
                  <img src="images/cse473.jpg" alt="cse473" style="width: 95%;">
                </td>
                <td height="0" valign="center">
                  <a href="https://courses.cs.washington.edu/courses/cse473/22au/">Undergraduate Teaching Assistant,
                    CSE473 Fall 2022</a>
                  <br>
                  <a href="https://courses.cs.washington.edu/courses/cse473/22sp/">Undergraduate Teaching Assistant,
                    CSE473 Spring 2022</a>
                </td>
              </tr>
              <tr>
                <td align="center" style="padding:20px;width:25%;vertical-align:middle"><img src="images/acl.png"
                    style="width: 75%;">
                  <h2>NAACL
                    <br>
                    2022
                  </h2>
                </td>
                <td width="75%" valign="center">
                  <a href="https://2022.naacl.org/">Student Volunteer, NAACL 2022</a>
                </td>
              </tr>
              <tr>
                <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/ictd.jpg" style="width: 90%;">
                  <h2>
                    2022</h2>
                </td>
                <td width="75%" valign="center">
                  <a href="https://ictd.org/">Student Volunteer, ICTD 2022</a>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>